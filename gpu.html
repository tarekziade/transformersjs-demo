<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ONNX Runtime with GPU</title>
    <link rel="stylesheet" href="styles.css" />
    <link rel="stylesheet" href="https://unpkg.com/chota@latest" />
    <script src="https://rawgit.com/mozilla/readability/master/Readability.js"></script>
    <link
      rel="stylesheet"
      href="https://unpkg.com/leaflet@1.7.1/dist/leaflet.css"
    />
    <script src="https://unpkg.com/leaflet@1.7.1/dist/leaflet.js"></script>
  </head>
  <body>
    <script type="module">
      import * as ort from "./ort.webgpu.min.js";

      const generateButton = document.getElementById("generate-button");
      const output = document.getElementById("output");

      generateButton.addEventListener("click", async () => {
        var result;
        try {
          // create a new session and load the specific model.
          //
          // the model in this example contains a single MatMul node
          // it has 2 inputs: 'a'(float32, 3x4) and 'b'(float32, 4x3)
          // it has 1 output: 'c'(float32, 3x3)
          const session = await ort.InferenceSession.create("./model.onnx");

          // prepare inputs. a tensor need its corresponding TypedArray as data
          const dataA = Float32Array.from([
            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
          ]);
          const dataB = Float32Array.from([
            10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120,
          ]);
          const tensorA = new ort.Tensor("float32", dataA, [3, 4]);
          const tensorB = new ort.Tensor("float32", dataB, [4, 3]);

          // prepare feeds. use model input names as keys.
          const feeds = { a: tensorA, b: tensorB };

          // feed inputs and run
          const results = await session.run(feeds);

          // read from results
          const dataC = results.c.data;

          result = `data of result tensor 'c': ${dataC}`;
        } catch (e) {
          result = `failed to inference ONNX model: ${e}.`;
        }
        output.innerHTML = result;
      });
    </script>

    <div class="container">
      <nav class="nav">
        <div class="nav-left">
          <a class="brand" href="#">GAI Demos</a>
          <div class="tabs">
            <a href="index.html">Summarize</a>
            <a href="ner.html">NER</a>
            <a href="classification.html">Group</a>
            <a href="topics.html">Topics</a>
            <a href="gpu.html" class="active">GPU</a>
          </div>
        </div>
      </nav>

      <div class="card" style="margin-top: 15px; margin-bottom: 15px">
        <header>
          <h4>onnxruntime with WebGPU enabled</h4>
        </header>
        <p>
          Loads a model and runs a MatMul on the GPU. To activate WebGPU make
          sure yo urun Nightly and toggle `gfx.webgpu.ignore-blocklist`,
          `dom.webgpu.workers.enabled` and `dom.webgpu.enabled` in about:config
        </p>

        <footer class="is-right">
          <button class="button primary" id="generate-button">Generate</button>
        </footer>
      </div>

      <div class="row" style="height: 100%">
        <div class="col-12" id="sidebar">
          <div id="output"></div>
          <div id="summary-stats"></div>
        </div>
      </div>
    </div>
  </body>
</html>
